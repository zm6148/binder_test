{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.stats.multitest as multi\n",
    "import scipy\n",
    "\n",
    "# create file system to interact with cloud storage\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre-defined functions\n",
    "\n",
    "# load peak list that is the mzlearn run results (use max)\n",
    "def load_peak_csv(blobs, mzlearn_path):\n",
    "    # create list of raw data files\n",
    "    peak_filenames = set()\n",
    "    for blob in blobs:\n",
    "        # only select the top folder structure\n",
    "        # split blobs by '/'\n",
    "        blob_splited = blob.name.split('/')\n",
    "        # index 4 is the filename if index 3 is peaks\n",
    "        if blob_splited[3] == 'peaks':\n",
    "            peak_filename = blob_splited[4]\n",
    "            # build the dict\n",
    "            if peak_filename not in peak_filenames:\n",
    "                peak_filenames.add(peak_filename)\n",
    "    # need to be consistent on which file to load\n",
    "    peak_filenames = list(peak_filenames)\n",
    "    peak_filenames.sort()\n",
    "    # print(peak_filenames)\n",
    "    # if there are 'ax', 'rea' in the file name then load the index 1\n",
    "    # else load the inde 7\n",
    "    if peak_filenames[0].split(\"-\")[0] == 'area':\n",
    "        index_to_load = 7\n",
    "    else:\n",
    "        index_to_load = 1\n",
    "    # create a list of all peaks found\n",
    "    # read from .csv file on bucket using panda\n",
    "    fs = gcsfs.GCSFileSystem(project='mzlearn-webapp')\n",
    "    # convert to list\n",
    "    peak_filenames = list(peak_filenames)\n",
    "    peak_filenames.sort()\n",
    "    # print(peak_filenames)\n",
    "    # 1 for max 4 for area\n",
    "    file_name = f'mzlearn-webapp.appspot.com/{mzlearn_path}/peaks/{peak_filenames[index_to_load]}'\n",
    "    # load peak info\n",
    "    with fs.open(file_name) as f:\n",
    "        df = pd.read_csv(f, index_col=False)\n",
    "    # add code there to detemine coloumns for file names\n",
    "    return df\n",
    "\n",
    "# function for hall mark normalization\n",
    "def hallmark_normalization(peak_table, first_data_col_idx, mzlearn_path, MVT):\n",
    "    # init file system\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    storage_client = storage.Client()\n",
    "    # load data\n",
    "    data, file_order, bboxes, fingerprint_idxs = load_data(peak_table,\n",
    "                                                           first_data_col_idx=first_data_col_idx,\n",
    "                                                           dim_signal=1,\n",
    "                                                           MVT=MVT)\n",
    "    # load the dict to all hallmarks path\n",
    "    dct_hallmarks_name = \"pruned_dct_hallmarks.pkl\"\n",
    "    dct_hallmark_path = f'mzlearn-webapp.appspot.com/{mzlearn_path}/maps/'\n",
    "    with fs.open(dct_hallmark_path + dct_hallmarks_name, \"rb\") as f:\n",
    "        dct_hallmarks = pickle.load(f)\n",
    "    print(f'found {len(dct_hallmarks)} hallmarks')\n",
    "    # hallmark normalization\n",
    "    for fn_text, fn in [('sum', lambda arr: np.sum(arr))]:\n",
    "        for corr_thresh in [0.9]:\n",
    "            dct_hallmarks_copy = restrain_hallmarks_to_score_above_thresh(\n",
    "                dct_hallmarks=copy.deepcopy(dct_hallmarks),\n",
    "                corr_thresh=corr_thresh)\n",
    "            dct_totals = defaultdict(lambda: 0)\n",
    "            for f in file_order:\n",
    "                for idx in dct_hallmarks_copy[f]:\n",
    "                    dct_totals[f] += fn(dct_hallmarks_copy[f][idx][3][:, 2])\n",
    "            data_post = data.copy()\n",
    "            assert data_post.shape[0] == len(list(dct_totals.keys()))\n",
    "            for i in range(data_post.shape[0]):\n",
    "                f = file_order[i]\n",
    "                data_post[i, :] = data_post[i, :] / dct_totals[f]\n",
    "            data_post[np.isnan(data_post)] = 0\n",
    "            data_post = np.log2(data_post + 1)\n",
    "    return data_post\n",
    "\n",
    "# read peak list data\n",
    "def load_data(table, first_data_col_idx, dim_signal, MVT=0):\n",
    "\n",
    "    # only keep non-pool samples i.e. patients\n",
    "    file_order = [f for f in table.columns[first_data_col_idx:] if not 'pool' in f.lower() and '.mzml' in f.lower()]\n",
    "    print(f'{len(file_order)} non-pool (patient) samples found')\n",
    "\n",
    "    data = torch.FloatTensor(table[file_order].astype('float32').to_numpy())\n",
    "    assert data.shape[0] % dim_signal == 0\n",
    "\n",
    "    bboxes = [[\n",
    "        table['rtmin'][i],\n",
    "        table['mzmin'][i],\n",
    "        table['rtmax'][i],\n",
    "        table['mzmax'][i]] for i in range(data.shape[0])]\n",
    "\n",
    "    if 'mzFingerprint_id' in table.columns:\n",
    "        fingerprint_idxs = table['mzFingerprint_id']\n",
    "\n",
    "    # get rid of any feature with MVs\n",
    "    mask = ((~torch.isnan(data)).sum(dim=-1) / data.shape[-1]) > MVT\n",
    "    data = data[mask]\n",
    "    bboxes = [b for i, b in enumerate(bboxes) if mask[i]]\n",
    "    if 'mzFingerprint_id' in table.columns:\n",
    "        fingerprint_idxs = [f for i, f in enumerate(fingerprint_idxs) if mask[i]]\n",
    "\n",
    "    data = data.detach().numpy().transpose()\n",
    "    print(f'data shape: {data.shape}')\n",
    "    print(f'{len(bboxes)} bboxes')\n",
    "    if 'mzFingerprint_id' in table.columns:\n",
    "        print(f'{len(fingerprint_idxs)} fingerprint idxs')\n",
    "    else:\n",
    "        fingerprint_idxs = None\n",
    "    print('\\n')\n",
    "    return data, file_order, bboxes, fingerprint_idxs\n",
    "\n",
    "# select only above corr_thresh hallmarks\n",
    "def restrain_hallmarks_to_score_above_thresh(dct_hallmarks, corr_thresh, list_mzMLs=None):\n",
    "    if list_mzMLs is None:\n",
    "        list_mzMLs = list(dct_hallmarks.keys())\n",
    "    else:\n",
    "        for f in list_mzMLs: assert f in dct_hallmarks\n",
    "\n",
    "    hallmark_idxs = list(dct_hallmarks[list_mzMLs[0]].keys())\n",
    "    num_initial = len(hallmark_idxs)\n",
    "\n",
    "    new_dct_hallmarks = {f: {} for f in list_mzMLs}\n",
    "\n",
    "    for hallmark_idx in hallmark_idxs:\n",
    "\n",
    "        if all([dct_hallmarks[f][hallmark_idx][2] > corr_thresh for f in list_mzMLs]):\n",
    "            for f in list_mzMLs:\n",
    "                new_dct_hallmarks[f][hallmark_idx] = dct_hallmarks[f][hallmark_idx]\n",
    "\n",
    "    found_ids = list(new_dct_hallmarks[list_mzMLs[0]].keys())\n",
    "    print(f'{len(found_ids)} signals with thresh {corr_thresh}')\n",
    "\n",
    "    return new_dct_hallmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load metadata\n",
      "load peaks from mzlearn run\n"
     ]
    }
   ],
   "source": [
    "## load data stored on the the cloud storage\n",
    "## metadata_df is the metadata data frame\n",
    "## peak_df is the mzlearn output peak list data frame\n",
    "\n",
    "mzlearn_path = 'mzlearn/NOVO_significant/2022-01-31 23:25:00'\n",
    "project_code = 'NASH_mouse_pos'\n",
    "\n",
    "print(\"load metadata\")\n",
    "metadata_path = f'mzlearn-webapp.appspot.com/{mzlearn_path}/feature_engine/{project_code}/metadata.pkl'\n",
    "with fs.open(metadata_path, 'rb') as handle:\n",
    "    metadata_df = pickle.load(handle)\n",
    "\n",
    "# load found peak list\n",
    "print(\"load peaks from mzlearn run\")\n",
    "blobs = storage_client.list_blobs('mzlearn-webapp.appspot.com', prefix=f'{mzlearn_path}/peaks')\n",
    "peak_df = load_peak_csv(blobs, mzlearn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 non-pool (patient) samples found\n",
      "data shape: (36, 6775)\n",
      "6775 bboxes\n",
      "6775 fingerprint idxs\n",
      "\n",
      "\n",
      "found 36 hallmarks\n",
      "269 signals with thresh 0.9\n",
      "normalized data from 36 files for 6775 found peaks\n",
      "peak_df updated with hallmark normalized Data\n"
     ]
    }
   ],
   "source": [
    "## do hallmark normalization\n",
    "\n",
    "# hard coded for now the column where file name starts\n",
    "first_data_col_idx = 10\n",
    "data_post = hallmark_normalization(peak_df, first_data_col_idx, mzlearn_path, 0)\n",
    "print(f'normalized data from {len(data_post)} files for {len(data_post[0])} found peaks')\n",
    "# updata the data frame with normalized intensity values starting from the first_data_col_idx till the end of file\n",
    "for idx in range(first_data_col_idx, first_data_col_idx + len(data_post)):\n",
    "    peak_df.iloc[:, idx] = data_post[idx - first_data_col_idx][:]\n",
    "print(\"peak_df updated with hallmark normalized Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalized, custom data analylsis can be performed to updated peak_df (mzlearn peak result list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d05900dc50d4cce92a7504257d308e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=5, description='mean', max=10), FloatSlider(value=1.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n",
    "import numpy as np\n",
    "import nbinteract as nbi\n",
    "\n",
    "def normal(mean, sd):\n",
    "    '''Returns 1000 points drawn at random fron N(mean, sd)'''\n",
    "    return np.random.normal(mean, sd, 1000)\n",
    "\n",
    "normal(10, 1.0)\n",
    "\n",
    "# Plot aesthetics\n",
    "options = {\n",
    "    'xlim': (-2, 12),\n",
    "    'ylim': (0, 0.7),\n",
    "    'bins': 20\n",
    "}\n",
    "\n",
    "# Pass in the `normal` function and let user change mean and sd.\n",
    "# Whenever the user interacts with the sliders, the `normal` function\n",
    "# is called and the returned data are plotted.\n",
    "nbi.hist(normal, mean=(0, 10), sd=(0, 2.0), options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
